{
    "run_id": "20250624_003635_train",
    "timestamp": "2025-06-24T00:36:35.217716",
    "model_type": "blstm",
    "preprocessing_info": {
        "directory": "processed\\train\\latest",
        "run_id": "20250624_001843_preprocess",
        "timestamp": "2025-06-24T00:18:43.043293",
        "sample_count": 3492
    },
    "config": {
        "test_size": 0.2,
        "random_state": 42,
        "model_specific": {
            "blstm_mfcc_count": 13,
            "lstm_units": 64,
            "dropout_rate": 0.3,
            "learning_rate": 0.001,
            "batch_size": 64,
            "epochs": 50
        }
    },
    "system_info": {
        "platform": "Windows-10-10.0.26100-SP0",
        "python_version": "3.10.0",
        "cpu_count": 20,
        "use_gpu_config": true,
        "gpu_available": false
    },
    "performance": {
        "accuracy": 0.9885550786838341,
        "training_time": 117.21869540214539,
        "class_report": {
            "Abdulbasit Abdulsamad": {
                "precision": 0.9927536231884058,
                "recall": 0.9856115107913669,
                "f1-score": 0.9891696750902527,
                "support": 139.0
            },
            "Abdulrahman Alsudaes": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 139.0
            },
            "Ahmad Al-Ajmy": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 139.0
            },
            "Maher Al Meaqli": {
                "precision": 0.9784172661870504,
                "recall": 0.9784172661870504,
                "f1-score": 0.9784172661870504,
                "support": 139.0
            },
            "Yasser Al-Dosari": {
                "precision": 0.9722222222222222,
                "recall": 0.9790209790209791,
                "f1-score": 0.975609756097561,
                "support": 143.0
            },
            "accuracy": 0.9885550786838341,
            "macro avg": {
                "precision": 0.9886786223195356,
                "recall": 0.9886099511998794,
                "f1-score": 0.9886393394749728,
                "support": 699.0
            },
            "weighted avg": {
                "precision": 0.988584451217405,
                "recall": 0.9885550786838341,
                "f1-score": 0.9885647781967043,
                "support": 699.0
            }
        }
    },
    "total_time": 117.70606899261475
}