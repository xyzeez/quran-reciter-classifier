{
    "run_id": "20250510_013651_train",
    "timestamp": "2025-05-10T01:36:51.743798",
    "model_type": "blstm",
    "preprocessing_info": {
        "directory": "processed\\train\\latest",
        "run_id": "20250509_221619_preprocess",
        "timestamp": "2025-05-09T22:16:19.715823",
        "sample_count": 17745
    },
    "config": {
        "test_size": 0.2,
        "random_state": 42,
        "model_specific": {
            "blstm_mfcc_count": 13,
            "lstm_units": 64,
            "dropout_rate": 0.3,
            "learning_rate": 0.001,
            "batch_size": 64,
            "epochs": 50
        }
    },
    "system_info": {
        "platform": "Windows-10-10.0.19045-SP0",
        "python_version": "3.9.13",
        "cpu_count": 4,
        "use_gpu_config": true,
        "gpu_available": false
    },
    "performance": {
        "accuracy": 0.9870386024232178,
        "training_time": 815.3097298145294,
        "class_report": {
            "Abdulbasit Abdulsamad": {
                "precision": 0.9957805907172996,
                "recall": 0.9971830985915493,
                "f1-score": 0.9964813511611541,
                "support": 710.0
            },
            "Mahmoud Khalil Al-Hussary": {
                "precision": 0.9591836734693877,
                "recall": 0.9943582510578279,
                "f1-score": 0.9764542936288089,
                "support": 709.0
            },
            "Mishary Alafasi": {
                "precision": 0.9971671388101983,
                "recall": 0.9915492957746479,
                "f1-score": 0.9943502824858758,
                "support": 710.0
            },
            "Mohammed Siddiq Al-Minshawi": {
                "precision": 0.9926793557833089,
                "recall": 0.9549295774647887,
                "f1-score": 0.9734386216798278,
                "support": 710.0
            },
            "Saud Al-Shuraim": {
                "precision": 0.9915966386554622,
                "recall": 0.9971830985915493,
                "f1-score": 0.9943820224719101,
                "support": 710.0
            },
            "accuracy": 0.9870386024232178,
            "macro avg": {
                "precision": 0.9872814794871314,
                "recall": 0.9870406642960727,
                "f1-score": 0.9870213142855153,
                "support": 3549.0
            },
            "weighted avg": {
                "precision": 0.9872893965922364,
                "recall": 0.9870386024232178,
                "f1-score": 0.9870242917497748,
                "support": 3549.0
            }
        }
    },
    "total_time": 816.9822301864624
}