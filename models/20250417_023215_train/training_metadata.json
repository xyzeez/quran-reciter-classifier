{
    "run_id": "20250417_023215_train",
    "timestamp": "2025-04-17T02:32:15.693979",
    "model_type": "blstm",
    "preprocessing_info": {
        "directory": "/content/processed/train/20250416_214513_preprocess",
        "run_id": "20250416_214513_preprocess",
        "timestamp": "2025-04-16T21:45:13.510371",
        "sample_count": 28392
    },
    "config": {
        "test_size": 0.2,
        "random_state": 42,
        "model_specific": {
            "blstm_mfcc_count": 13,
            "lstm_units": 64,
            "dropout_rate": 0.3,
            "learning_rate": 0.001,
            "batch_size": 64,
            "epochs": 50
        }
    },
    "system_info": {
        "platform": "Linux-6.1.85+-x86_64-with-glibc2.35",
        "python_version": "3.11.12",
        "cpu_count": 2,
        "use_gpu_config": true,
        "gpu_available": true
    },
    "performance": {
        "accuracy": 0.9827434407466104,
        "training_time": 151.83502769470215,
        "class_report": {
            "Abdulbasit Abdulsamad": {
                "precision": 0.9985704074338814,
                "recall": 0.9844961240310077,
                "f1-score": 0.9914833215046132,
                "support": 1419.0
            },
            "Mahmoud Khalil Al-Hussary": {
                "precision": 0.9755073477956613,
                "recall": 0.9816901408450704,
                "f1-score": 0.9785889785889786,
                "support": 1420.0
            },
            "Mishary Alafasi": {
                "precision": 0.9789029535864979,
                "recall": 0.9802816901408451,
                "f1-score": 0.9795918367346939,
                "support": 710.0
            },
            "Mohammed Siddiq Al-Minshawi": {
                "precision": 0.9753867791842475,
                "recall": 0.9767605633802817,
                "f1-score": 0.976073187895848,
                "support": 1420.0
            },
            "Saud Al-Shuraim": {
                "precision": 0.9846796657381616,
                "recall": 0.995774647887324,
                "f1-score": 0.9901960784313726,
                "support": 710.0
            },
            "accuracy": 0.9827434407466104,
            "macro avg": {
                "precision": 0.98260943074769,
                "recall": 0.9838006332569058,
                "f1-score": 0.9831866806311013,
                "support": 5679.0
            },
            "weighted avg": {
                "precision": 0.9828111865083036,
                "recall": 0.9827434407466104,
                "f1-score": 0.9827583253022376,
                "support": 5679.0
            }
        }
    },
    "total_time": 153.47525835037231
}