# Quran Reciter Classifier

A comprehensive machine learning system for identifying Quran reciters from audio recordings and identifying specific Quranic verses (Ayahs) using advanced audio processing, feature extraction, and classification techniques.

## Table of Contents

- [Overview](#overview)
- [System Architecture](#system-architecture)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
  - [Data Preparation](#data-preparation)
  - [Preprocessing Pipeline](#preprocessing-pipeline)
  - [Training Pipeline](#training-pipeline)
  - [Testing Pipeline](#testing-pipeline)
  - [Prediction Pipeline](#prediction-pipeline)
  - [Server/API Pipeline](#serverapi-pipeline)
- [Models](#models)
- [Audio Features](#audio-features)
- [API Documentation](#api-documentation)
- [Development Tools](#development-tools)
- [Data Management](#data-management)
- [Troubleshooting](#troubleshooting)
- [Acknowledgments](#Acknowledgments)

## Overview

The Quran Reciter Classifier is a dual-purpose machine learning system that:

1. **Reciter Identification**: Identifies Quran reciters from audio recordings using traditional ML (Random Forest) and deep learning (BLSTM) approaches
2. **Ayah Identification**: Identifies specific Quranic verses from audio using Whisper ASR and fuzzy text matching

The system supports both traditional machine learning and deep learning approaches, with comprehensive preprocessing, training, testing, and deployment pipelines.

## 🏗️ System Architecture

The system consists of five main pipelines:

1. **Preprocessing Pipeline**: Audio processing and feature extraction
2. **Training Pipeline**: Model training with Random Forest or BLSTM
3. **Testing Pipeline**: Model evaluation and performance analysis
4. **Prediction Pipeline**: Single audio file prediction
5. **Server Pipeline**: REST API for real-time inference

## Features

- **Multi-Model Support**: Random Forest and Bidirectional LSTM models
- **Comprehensive Audio Processing**: Noise reduction, normalization, and feature extraction
- **Advanced Feature Engineering**: MFCCs, spectral features, rhythm features, and tonal features
- **Data Augmentation**: Pitch shifting, time stretching, noise addition, and volume adjustment
- **Reliability Analysis**: Distance-based prediction verification and confidence scoring
- **REST API**: Flask-based server with endpoints for both reciter and Ayah identification
- **Extensive Evaluation**: Confusion matrices, performance metrics, and visualization tools
- **GPU Support**: CUDA acceleration for faster processing
- **Robust Logging**: Comprehensive logging and progress tracking with Rich console output
- **Modular Design**: Extensible architecture with clear separation of concerns
- **Debug Mode**: Enhanced debugging with request/response logging and detailed error traces

## Project Structure

```
.
├── api-responses/              # API response cache and debug data (generated)
│   ├── getReciter/            # Reciter identification debug data
│   └── getAyah/               # Ayah identification debug data
├── config/                    # Configuration settings
│   ├── __init__.py           # Configuration package initializer
│   └── config.py             # Main configuration parameters
├── data/                      # Input data directories
│   ├── train/                # Training audio files (organized by reciter)
│   ├── test/                 # Test audio files (organized by reciter)
│   ├── test-cases/           # Test case data for API validation
│   │   ├── tc-001/          # Clean reciter audio
│   │   ├── tc-002/          # Non-training reciters
│   │   ├── tc-003/          # Ayah identification (SSSAAA.mp3 format)
│   │   └── tc-004/ to tc-010/ # Additional test scenarios
│   ├── quran.json            # Quran text data for Ayah identification
│   ├── reciters.json         # Reciter metadata for training models (generated)
│   ├── recitersAll.json      # Complete reciter database
│   ├── surahs.json          # Surah information
│   └── dataset_splits.json   # Training/testing split configuration
├── dataset/                   # Downloaded dataset (generated by download tools)
├── processed/                 # Preprocessed data (generated)
│   ├── train/                # Training data processing outputs
│   │   ├── latest/           # Symlink to most recent preprocessing
│   │   └── YYYYMMDD_HHMMSS_preprocess/  # Timestamped preprocessing runs
│   └── test/                 # Test data processing outputs
├── models/                    # Trained models (generated)
│   ├── latest/               # Symlink to most recent model
│   └── YYYYMMDD_HHMMSS_train/  # Timestamped training runs
│       ├── model_random_forest.joblib  # Model file
│       ├── training_metadata.json      # Training configuration
│       ├── training_summary.txt        # Human-readable summary
│       └── visualizations/             # Training visualizations
├── test_results/              # Test results (generated)
│   └── YYYYMMDD_HHMMSS_test/  # Timestamped test runs
│       ├── detailed_results.csv        # Per-file test results
│       ├── summary_report.json         # Test metrics
│       ├── test_summary.txt           # Human-readable summary
│       └── confusion_matrix_*.png     # Confusion matrices
├── test-cases-report/         # Test case runner outputs (generated)
│   └── YYYYMMDD_HHMMSS_test/  # Test case execution results
├── logs/                      # Log files (generated)
├── server/                    # Flask API server
│   ├── __init__.py           # Server package initializer
│   ├── app.py                # Main server entry point
│   ├── app_factory.py        # Flask application factory
│   ├── config.py             # Server-specific configuration
│   ├── requirements.txt      # Server dependencies
│   ├── routes/               # API endpoint blueprints
│   │   ├── ayah.py          # Ayah identification endpoints
│   │   ├── reciter.py       # Reciter identification endpoints
│   │   ├── health.py        # Health check endpoints
│   │   └── models.py        # Model information endpoints
│   ├── services/             # Business logic layer
│   │   ├── ayah_service.py  # Ayah identification service
│   │   └── reciter_service.py  # Reciter identification service
│   └── utils/                # Server utility functions
│       ├── audio_processor_matcher.py  # Audio processing utilities
│       ├── logging_config.py           # Rich logging configuration
│       ├── model_loader.py             # Model loading utilities
│       ├── quran_data.py              # Quran data management
│       ├── quran_matcher.py           # Whisper-based Ayah matching
│       └── text_utils.py              # Text processing utilities
├── src/                       # Core ML pipeline source code
│   ├── data/                 # Data handling modules
│   │   ├── __init__.py      # Data package exports
│   │   ├── augmentation.py  # Audio data augmentation
│   │   ├── loader.py        # Audio file loading
│   │   └── preprocessing.py # Audio preprocessing
│   ├── features/             # Feature extraction
│   │   ├── __init__.py      # Features package exports
│   │   └── extractors.py    # Audio feature extraction
│   ├── models/               # ML model implementations
│   │   ├── __init__.py      # Models package exports
│   │   ├── base_model.py    # Abstract base model class
│   │   ├── random_forest.py # Random Forest implementation
│   │   ├── blstm_model.py   # BLSTM implementation
│   │   └── model_factory.py # Model creation and loading
│   ├── pipelines/            # ML pipeline implementations
│   │   ├── __init__.py      # Pipelines package exports
│   │   ├── preprocess_pipeline.py  # Preprocessing pipeline
│   │   ├── train_pipeline.py       # Training pipeline
│   │   ├── test_pipeline.py        # Testing pipeline
│   │   └── predict_pipeline.py     # Prediction pipeline
│   ├── evaluation/           # Model evaluation tools
│   │   ├── __init__.py      # Evaluation package exports
│   │   ├── metrics.py       # Performance metrics
│   │   └── visualization.py # Evaluation visualizations
│   └── utils/                # Utility functions
│       ├── __init__.py      # Utils package exports
│       ├── distance_utils.py    # Distance calculations and reliability
│       ├── gpu_utils.py         # GPU detection utilities
│       └── logging_utils.py     # Logging utilities
├── scripts/                   # Entry point scripts
│   ├── __init__.py           # Scripts package initializer
│   ├── preprocess.py         # Preprocessing script entry point
│   ├── train.py              # Training script entry point
│   ├── test.py               # Testing script entry point
│   └── predict.py            # Prediction script entry point
├── tools/                     # Project management tools
│   ├── download_dataset.py   # Dataset download utility
│   ├── prepare_data.py       # Data preparation utility
│   └── test_case_runner.py   # Comprehensive API testing tool
├── requirements.txt           # Main project dependencies
└── README.md                 # This file
```

## Installation

### Prerequisites

- Python 3.8+
- FFmpeg (for audio processing)
- GPU with CUDA support (optional, for acceleration)

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd quran-reciter-classifier
```

### Step 2: Install FFmpeg

FFmpeg is required for audio processing:

```bash
# Ubuntu/Debian
sudo apt update && sudo apt install ffmpeg

# macOS with Homebrew
brew install ffmpeg

# Windows - Download from https://ffmpeg.org/download.html
# Add to PATH after installation

# Verify installation
ffmpeg -version
```

### Step 3: Create Virtual Environment

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS/Linux
source .venv/bin/activate
```

### Step 4: Install Dependencies

```bash
# Install PyTorch with CUDA support (optional but recommended)
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install main project dependencies
pip install -r requirements.txt

# Install server-specific dependencies (if using API)
pip install -r server/requirements.txt
```

### Step 5: Verify Installation

```bash
# Test PyTorch CUDA
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# Test audio processing
python -c "import librosa; print('Audio processing ready')"

# Test Whisper
python -c "import whisper; print('Whisper ready')"
```

### Step 6: Prepare Data Structure

```bash
# Create necessary directories
mkdir -p data/train data/test data/test-cases

# Place audio files in appropriate directories:
# data/train/{reciter_name}/*.mp3
# data/test/{reciter_name}/*.mp3
```

## Configuration

The system uses multiple configuration files:

### Main Configuration (`config/config.py`)

Key parameters include:

```python
# Directories
TRAIN_DATA_DIR = "data/train"
TEST_DATA_DIR = "data/test"
MODEL_OUTPUT_DIR = "models"

# Audio Processing
DEFAULT_SAMPLE_RATE = 22050
RECITER_MIN_DURATION = 5.0   # Minimum audio duration in seconds
RECITER_MAX_DURATION = 30.0  # Maximum audio duration in seconds

# Feature Extraction
N_MFCC = 32                  # Number of MFCC features
BLSTM_MFCC_COUNT = 13        # MFCCs used for BLSTM (first 13)
N_MEL_BANDS = 64
N_CHROMA = 12

# Model Parameters
MODEL_TYPE = 'random_forest'  # Default model type
N_ESTIMATORS = 100           # Random Forest trees
LSTM_UNITS = 64              # BLSTM units
DROPOUT_RATE = 0.3           # General dropout rate
BLSTM_DROPOUT_RATE = 0.5     # Enhanced dropout for BLSTM

# Reliability Parameters
CONFIDENCE_THRESHOLD = 0.95
SECONDARY_CONFIDENCE_THRESHOLD = 0.10
MAX_CONFIDENCE_DIFF = 0.80
MAX_DISTANCE_RATIO_THRESHOLD = 1.0

# GPU Usage
USE_GPU = True               # Enable GPU if available
```

### Server Configuration (`server/config.py`)

Server-specific settings:

```python
# Network Settings
HOST = '0.0.0.0'
PORT = 5000

# Model Loading
MODEL_ID = '20250417_023215_train'  # Specific model to load
MODEL_DIR = Path(MODEL_OUTPUT_DIR)
LATEST_MODEL_SYMLINK = MODEL_DIR / 'latest'

# Audio Constraints
MIN_AUDIO_DURATION = 5.0     # Minimum audio duration for processing
MAX_AUDIO_DURATION = 30.0    # Maximum audio duration for processing
SAMPLE_RATE = 22050          # Target sample rate

# Whisper Settings
WHISPER_MODEL_ID = "tarteel-ai/whisper-base-ar-quran"

# Ayah Matching Defaults
AYAH_DEFAULT_MAX_MATCHES = 5
AYAH_DEFAULT_MIN_CONFIDENCE = 0.70

# Response Settings
TOP_N_PREDICTIONS = 5        # Number of top predictions to return
```

## Usage

### Data Preparation

#### Option 1: Download Dataset (Automated)

```bash
# Download audio files from configured sources
python tools/download_dataset.py

# Prepare training/testing splits
python tools/prepare_data.py
```

#### Option 2: Manual Data Organization

Place audio files in the following structure:

```
data/
├── train/
│   ├── mishary_alafasy/
│   │   ├── 001001.mp3    # Surah 1, Ayah 1
│   │   └── 001002.mp3    # Surah 1, Ayah 2
│   └── abdul_rahman_sudais/
│       ├── 001001.mp3
│       └── 001002.mp3
├── test/
│   ├── known_reciter/
│   │   └── test_audio.mp3
│   └── unknown_reciter/
│       └── test_audio.mp3
└── test-cases/
    ├── tc-001/              # Clean reciter audio
    ├── tc-003/              # Ayah files named SSSAAA.mp3
    │   ├── 001001.mp3       # Surah 1, Ayah 1
    │   └── 002255.mp3       # Surah 2, Ayah 255
    └── tc-008/              # Silence/non-speech audio
```

### Preprocessing Pipeline

**Entry Point**: `scripts/preprocess.py`  
**Core Implementation**: `src/pipelines/preprocess_pipeline.py`

#### Command Syntax

```bash
python scripts/preprocess.py [--mode {train,test}] [--no-augment]
```

#### Arguments

- `--mode`: Processing mode (`train` or `test`) - Default: `train`
- `--no-augment`: Skip data augmentation (faster processing, reduced diversity)

#### Examples

```bash
# Preprocess training data with augmentation
python scripts/preprocess.py --mode train

# Preprocess test data (no augmentation)
python scripts/preprocess.py --mode test

# Preprocess training data without augmentation (faster)
python scripts/preprocess.py --mode train --no-augment
```

#### Process Flow

1. **Audio Loading**: Load MP3/WAV files using librosa with error handling
2. **Noise Reduction**: Apply spectral noise reduction using noisereduce
3. **Normalization**: Amplitude normalization and silence trimming
4. **Resampling**: Convert to 22050 Hz sample rate
5. **Feature Extraction**: Extract ~350 comprehensive audio features
6. **Data Augmentation** (train mode only):
   - Pitch shifting (±1.0 semitones)
   - Time stretching (0.95x, 1.05x rates)
   - Noise addition (0.003 factor)
   - Volume adjustment (0.9x scaling)

#### Outputs

**Location**: `processed/{mode}/YYYYMMDD_HHMMSS_preprocess/`

- `all_features.npy`: Extracted features array (samples × ~350 features)
- `all_metadata.csv`: File metadata and labels
- `preprocessing_metadata.json`: Run configuration and statistics
- `file_inventory.csv`: Processing status per file
- `file_feature_map.csv`: Feature index mapping
- `reciter_profiles.json`: Per-reciter processing statistics
- `preprocessing_summary.txt`: Human-readable summary
- `feature_statistics.json`: Feature distribution statistics
- `visualizations/`: Feature distribution and correlation plots

**Symlink**: `processed/{mode}/latest` → most recent run

### Training Pipeline

**Entry Point**: `scripts/train.py`  
**Core Implementation**: `src/pipelines/train_pipeline.py`

#### Command Syntax

```bash
python scripts/train.py [--model-type {random_forest,blstm}] [--preprocess-file-id PREPROCESS_FILE_ID]
```

#### Arguments

- `--model-type`: Model architecture (`random_forest` or `blstm`) - Default: from config
- `--preprocess-file-id`: Specific preprocessing run ID (e.g., `20240306_143208_preprocess`) - Default: latest

#### Examples

```bash
# Train Random Forest with latest preprocessing
python scripts/train.py --model-type random_forest

# Train BLSTM with specific preprocessing run
python scripts/train.py --model-type blstm --preprocess-file-id 20240306_143208_preprocess

# Train with default model type from config
python scripts/train.py
```

#### Process Flow

1. **Data Loading**: Load preprocessed features and metadata
2. **Train/Validation Split**: 80/20 split with stratification
3. **Model Training**:
   - **Random Forest**: 100 estimators, max depth 10, full feature set
   - **BLSTM**: First 13 MFCCs, sequence creation, bidirectional processing
4. **Reliability Metrics**: Calculate centroids and distance thresholds
5. **Model Evaluation**: Performance metrics and validation
6. **Visualization**: Confusion matrices and feature importance

#### Model Architectures

**Random Forest**:

- 100 estimators with max depth 10
- Uses all ~350 extracted features
- Calibrated probabilities using sigmoid method
- Feature importance analysis
- Cross-validation with 5 folds

**BLSTM (Bidirectional LSTM)**:

- Uses first 13 MFCC coefficients (configurable via `BLSTM_MFCC_COUNT`)
- Sequence length: 16 frames (configurable via `BLSTM_SEQUENCE_LENGTH`)
- 64 LSTM units per direction (128 total)
- Simple attention mechanism for temporal focus
- 0.5 dropout rate for enhanced regularization
- 128 dense units in classification layer
- Learning rate: 0.0005 with weight decay: 0.01
- Early stopping with patience: 10 epochs

#### Outputs

**Location**: `models/YYYYMMDD_HHMMSS_train/`

- `model_{type}.joblib`: Trained model file
- `training_metadata.json`: Complete training configuration and results
- `training_summary.txt`: Human-readable training summary
- `visualizations/confusion_matrix.png`: Training confusion matrix
- `visualizations/feature_importance.png`: Feature importance (Random Forest only)

**Symlink**: `models/latest` → most recent training run

### Testing Pipeline

**Entry Point**: `scripts/test.py`  
**Core Implementation**: `src/pipelines/test_pipeline.py`

#### Command Syntax

```bash
python scripts/test.py [--model-file-id MODEL_FILE_ID] [--list-models] [--list-tests]
```

#### Arguments

- `--model-file-id`: Training run ID (e.g., `20240306_152417_train`) - Default: latest
- `--list-models`: List all available trained models
- `--list-tests`: List all previous test runs

#### Examples

```bash
# Test latest model
python scripts/test.py

# Test specific model
python scripts/test.py --model-file-id 20240306_152417_train

# List available models
python scripts/test.py --list-models

# List previous test results
python scripts/test.py --list-tests
```

#### Process Flow

1. **Model Loading**: Load specified or latest trained model
2. **Test Data Loading**: Load preprocessed test features
3. **Prediction Generation**: Generate predictions for all test samples
4. **Reliability Analysis**: Analyze prediction confidence and distance metrics
5. **Performance Evaluation**: Calculate accuracy, precision, recall, F1-score
6. **Confusion Matrices**: Separate matrices for training vs non-training reciters
7. **Statistical Analysis**: Per-reciter performance breakdown

#### Evaluation Metrics

- **Overall Accuracy**: Correct predictions / Total predictions
- **Reliability Rate**: Reliable predictions / Total predictions
- **False Positive Rate**: Misidentified unknown reciters as known
- **Per-Reciter Statistics**: Individual reciter performance analysis

#### Outputs

**Location**: `test_results/YYYYMMDD_HHMMSS_test/`

- `detailed_results.csv`: Per-file prediction results
- `summary_report.json`: Complete test metrics
- `test_summary.txt`: Human-readable test summary
- `test_metadata.json`: Test configuration and system info
- `confusion_matrix_training_reciters.png`: Known reciters performance
- `confusion_matrix_nontraining_reciters.png`: Unknown reciters handling

### Prediction Pipeline

**Entry Point**: `scripts/predict.py`  
**Core Implementation**: `src/pipelines/predict_pipeline.py`

#### Command Syntax

```bash
python scripts/predict.py --audio AUDIO_FILE [--model-file-id MODEL_FILE_ID] [--true-label RECITER_NAME] [--list-models]
```

#### Arguments

- `--audio`: Path to audio file (required)
- `--model-file-id`: Training run ID - Default: latest
- `--true-label`: True reciter name for verification (optional)
- `--list-models`: List available models

#### Examples

```bash
# Predict reciter for audio file
python scripts/predict.py --audio path/to/audio.mp3

# Predict with verification
python scripts/predict.py --audio path/to/audio.mp3 --true-label "Mishary Alafasy"

# Use specific model
python scripts/predict.py --audio path/to/audio.mp3 --model-file-id 20240306_152417_train

# List available models
python scripts/predict.py --list-models
```

#### Process Flow

1. **Audio Loading**: Load and validate audio file
2. **Preprocessing**: Apply same preprocessing as training pipeline
3. **Feature Extraction**: Extract audio features using same methods
4. **Prediction**: Generate reciter prediction with confidence scores
5. **Reliability Analysis**: Analyze prediction reliability using distance metrics
6. **Visualization**: Generate prediction analysis plots

#### Outputs

**Location**: `logs/prediction_results_YYYYMMDD_HHMMSS/`

- `prediction_analysis.png`: Confidence and distance analysis visualization
- `prediction_summary.json`: Prediction results and metadata

### Server/API Pipeline

**Entry Point**: `server/app.py`  
**Core Implementation**: `server/app_factory.py`

#### Command Syntax

```bash
python -m server.app [--host HOST] [--port PORT] [--debug]
```

#### Arguments

- `--host`: Hostname to bind (default: 0.0.0.0)
- `--port`: Port to bind (default: 5000)
- `--debug`: Enable debug mode with enhanced logging and request saving

#### Examples

```bash
# Start server with defaults
python -m server.app

# Start with custom host and port
python -m server.app --host localhost --port 8080

# Start in debug mode (recommended for development)
python -m server.app --debug
```

#### Initialization Process

1. **Logging Setup**: Configure Rich logging with progress bars and colored output
2. **Quran Data Loading**: Load Quran text data for Ayah identification
3. **Model Loading**: Initialize reciter identification model
4. **Whisper Initialization**: Setup QuranMatcher with Whisper ASR model
5. **Blueprint Registration**: Register API endpoints and middleware

#### Debug Mode Features

- Enhanced logging with stack traces and detailed timing
- Debug data saving for requests/responses in `api-responses/`
- Request timing information in response headers
- Detailed error reporting with full context

## Models

### Random Forest

A robust ensemble learning approach optimized for voice characteristics:

- **Architecture**: 100 decision trees with max depth 10
- **Features**: Full feature set (~350 features including MFCCs, spectral, temporal, and tonal features)
- **Calibration**: Sigmoid probability calibration for better confidence estimates
- **Validation**: 5-fold cross-validation during training
- **Advantages**: Fast training/inference, interpretable, robust to overfitting, excellent baseline performance

### BLSTM (Bidirectional Long Short-Term Memory)

A deep learning approach optimized for temporal audio patterns:

- **Architecture**: Bidirectional LSTM with attention mechanism
- **Input Processing**: First 13 MFCC coefficients arranged in sequences of length 16
- **LSTM Configuration**: 64 units per direction (128 total bidirectional units)
- **Attention**: Simple attention layer for temporal focus across sequence
- **Regularization**: 0.5 dropout rate and L2 weight decay (0.01)
- **Training**: Adam optimizer with learning rate scheduling and early stopping
- **Advantages**: Captures temporal patterns, attention mechanism highlights important time segments, state-of-the-art performance for audio sequences

### Reliability Analysis

Both models use sophisticated distance-based reliability verification:

- **Centroids**: Calculate class centroids in feature space during training
- **Thresholds**: 95th percentile of intra-class distances for each reciter
- **Multi-Criteria Verification**: Prediction reliability determined by:
  - Primary confidence ≥ 95%
  - Secondary confidence < 10%
  - Confidence difference ≥ 80%
  - Distance to centroid ratio ≤ 1.0
- **Unknown Handling**: Unreliable predictions marked as "Unknown" to prevent false positives

## Audio Features

The system extracts comprehensive audio features optimized for voice identification:

### Core Features - MFCC Analysis (32 coefficients)

- **MFCCs**: Mel-Frequency Cepstral Coefficients capturing vocal tract characteristics
- **Delta MFCCs**: First-order derivatives capturing temporal changes in voice
- **Delta-Delta MFCCs**: Second-order derivatives capturing acceleration patterns

### Spectral Features

- **Chroma (12 features)**: Pitch class profiles for tonal analysis and maqam recognition
- **Mel Spectrograms (64 bands)**: Perceptually-motivated frequency representation
- **Spectral Contrast**: Difference between peaks and valleys in spectrum (7 bands)
- **Spectral Rolloff**: Frequency below which 85% of energy is concentrated
- **Spectral Centroid**: Center of mass of the spectrum (brightness measure)

### Temporal and Rhythm Features

- **Zero Crossing Rate**: Rate of sign changes in the signal (roughness indicator)
- **RMS Energy**: Root mean square energy measure (loudness proxy)
- **Tempogram**: Rhythm and tempo analysis for recitation style identification

### Tonal and Harmonic Features

- **Tonnetz (6 features)**: Tonal centroid features for harmonic analysis

### Feature Processing Pipeline

- **Normalization**: All features normalized to unit variance for consistent scaling
- **Aggregation**: Mean values computed across time frames to create fixed-length vectors
- **Total Dimensions**: Approximately 350+ features per audio sample
- **BLSTM Subset**: BLSTM model uses first 13 MFCCs arranged in temporal sequences

## API Documentation

### Base URL

```
http://localhost:5000
```

### Endpoints

#### POST /getReciter

Identify Quran reciter from audio file using machine learning models.

**Request**:

```bash
curl -X POST http://localhost:5000/getReciter \
  -F "audio=@reciter_audio.mp3" \
  -F "show_unreliable_predictions=false"
```

**Parameters**:

- `audio`: Audio file (MP3/WAV, 5-30 seconds recommended, 22050 Hz optimal)
- `show_unreliable_predictions`: Show results even if unreliable (default: false)

**Response**:

```json
{
  "reliable": true,
  "main_prediction": {
    "name": "Mishary Alafasy",
    "confidence": 95.5,
    "nationality": "Kuwait",
    "serverUrl": "https://server.example.com",
    "flagUrl": "https://flags.example.com/kw.png",
    "imageUrl": "https://images.example.com/mishary.jpg"
  },
  "top_predictions": [
    {
      "name": "Mishary Alafasy",
      "confidence": 95.5,
      "nationality": "Kuwait",
      "serverUrl": "https://server.example.com",
      "flagUrl": "https://flags.example.com/kw.png",
      "imageUrl": "https://images.example.com/mishary.jpg"
    },
    {
      "name": "Abdul Rahman Al-Sudais",
      "confidence": 3.2,
      "nationality": "Saudi Arabia",
      "serverUrl": "https://server2.example.com",
      "flagUrl": "https://flags.example.com/sa.png",
      "imageUrl": "https://images.example.com/sudais.jpg"
    }
  ],
  "response_time_ms": 1250.5
}
```

**Debug Mode Response** (when server started with `--debug`):

```json
{
  "reliable": true,
  "main_prediction": {...},
  "top_predictions": [...],
  "debug_info": {
    "feature_count": 350,
    "processing_steps": {
      "audio_loading": "success",
      "preprocessing": "success",
      "feature_extraction": "success",
      "prediction": "success"
    },
    "model_info": {
      "model_type": "RandomForest",
      "model_id": "20240306_152417_train"
    },
    "reliability_analysis": {
      "distance_ratio": 0.75,
      "confidence_gap": 0.92
    }
  },
  "response_time_ms": 1250.5
}
```

#### POST /getAyah

Identify Quranic verse from audio recording using Whisper ASR and fuzzy matching.

**Request**:

```bash
curl -X POST http://localhost:5000/getAyah \
  -F "audio=@ayah_audio.mp3" \
  -F "max_matches=5" \
  -F "min_confidence=0.70"
```

**Parameters**:

- `audio`: Audio file (MP3/WAV, 3-15 seconds recommended)
- `max_matches`: Maximum number of matches to return (default: 5)
- `min_confidence`: Minimum confidence threshold 0.0-1.0 (default: 0.70)

**Response**:

```json
{
  "matches_found": true,
  "total_matches": 3,
  "matches": [
    {
      "surah_number": "١٠٥",
      "surah_number_en": 105,
      "surah_name": "الفيل",
      "surah_name_en": "Al-Fil",
      "ayah_number": "٥",
      "ayah_number_en": 5,
      "ayah_text": "فَجَعَلَهُمۡ كَعَصۡفٖ مَّأۡكُولِۭ",
      "confidence_score": 77.49,
      "unicode": "🐘"
    },
    {
      "surah_number": "١٠٥",
      "surah_number_en": 105,
      "surah_name": "الفيل",
      "surah_name_en": "Al-Fil",
      "ayah_number": "٤",
      "ayah_number_en": 4,
      "ayah_text": "تَرۡمِيهِم بِحِجَارَةٖ مِّن سِجِّيلٖ",
      "confidence_score": 65.32,
      "unicode": "🐘"
    }
  ],
  "best_match": {
    "surah_number": "١٠٥",
    "surah_number_en": 105,
    "surah_name": "الفيل",
    "surah_name_en": "Al-Fil",
    "ayah_number": "٥",
    "ayah_number_en": 5,
    "ayah_text": "فَجَعَلَهُمۡ كَعَصۡفٖ مَّأۡكُولِۭ",
    "confidence_score": 77.49,
    "unicode": "🐘"
  },
  "response_time_ms": 2150.3
}
```

**Debug Mode Response** (when server started with `--debug`):

```json
{
  "matches_found": true,
  "total_matches": 3,
  "matches": [...],
  "best_match": {...},
  "transcription": "فَجَعَلَهُمْ كَعَصْفٍ مَأْكُول",
  "debug_info": {
    "transcription": "فَجَعَلَهُمْ كَعَصْفٍ مَأْكُول",
    "normalized_transcription": "فجعلهم كعصف ماكول",
    "normalized_matches": [
      {
        "normalized_text": "فجعلهم كعصف ماكول",
        "score": 77.49,
        "surah": 105,
        "ayah": 5
      }
    ],
    "whisper_model": "tarteel-ai/whisper-base-ar-quran",
    "processing_device": "CUDA"
  },
  "response_time_ms": 2150.3
}
```

#### GET /getAllReciters

Get list of all known reciters from the system's training database.

**Request**:

```bash
curl -X GET http://localhost:5000/getAllReciters
```

**Response**:

```json
{
  "reciters": [
    {
      "name": "Mishary Alafasy",
      "nationality": "Kuwait",
      "flagUrl": "https://flags.example.com/kw.png",
      "imageUrl": "https://images.example.com/mishary.jpg",
      "serverUrl": "https://server.example.com"
    },
    {
      "name": "Abdul Rahman Al-Sudais",
      "nationality": "Saudi Arabia",
      "flagUrl": "https://flags.example.com/sa.png",
      "imageUrl": "https://images.example.com/sudais.jpg",
      "serverUrl": "https://server2.example.com"
    }
  ]
}
```

#### GET /health

Check the health status of all system components including models and services.

**Request**:

```bash
curl -X GET http://localhost:5000/health
```

**Response**:

```json
{
  "status": "ok",
  "services": {
    "reciter_model": "loaded",
    "quran_data": "loaded",
    "quran_matcher": "initialized"
  }
}
```

**Error Response** (if services unavailable):

```json
{
  "status": "error",
  "services": {
    "reciter_model": "error",
    "quran_data": "loaded",
    "quran_matcher": "error"
  }
}
```

#### GET /models

Get detailed information about loaded models and their configurations.

**Request**:

```bash
curl -X GET http://localhost:5000/models
```

**Response**:

```json
{
  "reciter_model": {
    "status": "loaded",
    "details": {
      "model_type": "RandomForest",
      "model_id": "20240306_152417_train",
      "training_parameters": {
        "n_samples": 5000,
        "n_features": 350,
        "n_classes": 10,
        "classes": [
          "Mishary Alafasy",
          "Abdul Rahman Al-Sudais",
          "Maher Al Mueaqly"
        ],
        "training_info": {
          "cross_validation_mean": 0.94,
          "training_duration": 125.5,
          "start_time": "2024-03-06 15:24:17",
          "end_time": "2024-03-06 15:26:22"
        },
        "model_parameters": {
          "n_estimators": 100,
          "max_depth": 10,
          "random_state": 42
        }
      }
    }
  },
  "ayah_matcher": {
    "status": "initialized",
    "details": {
      "matcher_type": "Whisper",
      "whisper_model_id": "tarteel-ai/whisper-base-ar-quran",
      "device": "CUDA",
      "normalized_verses_count": 6236,
      "all_verses_count": 6236,
      "quran_data_source_surahs": 114,
      "service_defaults": {
        "max_matches": 5,
        "min_confidence": 0.7
      }
    }
  }
}
```

### Error Responses

All endpoints return appropriate HTTP status codes and descriptive error messages:

```json
{
  "error": "No audio file provided. Key must be 'audio'."
}
```

**Common Status Codes**:

- `200`: Success
- `400`: Bad Request (invalid input, missing parameters, audio too short/long)
- `500`: Internal Server Error (processing failure, model error)
- `503`: Service Unavailable (model not loaded, dependencies missing)

**Audio File Requirements**:

- **Formats**: MP3, WAV, M4A
- **Duration**: 5-30 seconds for reciter ID, 3-15 seconds for Ayah ID
- **Sample Rate**: 22050 Hz optimal (auto-converted if different)
- **Channels**: Mono preferred (stereo auto-converted)
- **Quality**: Clear speech, minimal background noise

## Development Tools

### Test Case Runner

Comprehensive API testing tool for validating system performance across various scenarios:

```bash
python tools/test_case_runner.py [--test-case TC_ID] [--endpoint URL]
```

#### Available Test Cases

- **tc-001**: Reciter ID with clean, high-quality audio
- **tc-002**: Non-training reciter handling (clean audio) - Tests unknown reciter rejection
- **tc-003**: Ayah ID with clean audio using SSSAAA.mp3 naming convention
- **tc-004**: Reciter ID with noisy/degraded audio quality
- **tc-005**: Non-training reciter handling with noisy audio
- **tc-006**: Ayah ID with noisy/degraded audio quality
- **tc-007**: Reciter ID with very short audio clips (2-4 seconds)
- **tc-008**: Reciter ID with silence/non-speech audio - Tests false positive prevention
- **tc-009**: Ayah ID with very short audio clips
- **tc-010**: Ayah ID with silence/non-speech audio

#### Test Data Structure

```bash
data/test-cases/
├── tc-001/                    # Clean reciter audio
│   ├── mishary_alafasy/
│   │   ├── sample1.mp3
│   │   └── sample2.mp3
│   └── abdul_rahman_sudais/
├── tc-002/                    # Non-training reciters
│   ├── unknown_reciter_1/
│   └── unknown_reciter_2/
├── tc-003/                    # Ayah identification
│   ├── 001001.mp3            # Surah 1, Ayah 1
│   ├── 002255.mp3            # Surah 2, Ayah 255 (Ayat al-Kursi)
│   └── 112001.mp3            # Surah 112, Ayah 1
└── tc-008/                    # Silence/non-speech
    ├── silence/
    │   ├── silence_5s.mp3
    │   └── silence_10s.mp3
    └── non_speech/
        ├── music.mp3
        └── office_noise.mp3
```

#### Example Usage

```bash
# Run all test cases against local server
python tools/test_case_runner.py --endpoint http://localhost:5000

# Run specific test case
python tools/test_case_runner.py --test-case tc-001 --endpoint http://localhost:5000

# Test against remote server
python tools/test_case_runner.py --endpoint https://api.example.com
```

#### Test Outputs

**Location**: `test-cases-report/YYYYMMDD_HHMMSS_test/`

**Generated Files**:

- `model_info.json`: Model configuration captured at test time
- `tc-001/results_tc-001.csv`: Detailed per-file results
- `tc-001/summary_tc-001.txt`: Human-readable test summary
- Individual directories for each test case with specific metrics

**Key Metrics Tracked**:

- **Accuracy**: Correct predictions vs total predictions
- **Reliability Rate**: Percentage of predictions marked as reliable
- **False Positive Rate**: Unknown reciters incorrectly identified as known
- **Response Times**: Server performance metrics
- **Error Rates**: Failed processing attempts

### Dataset Management Tools

#### Download Dataset

Automated dataset download from configured reciter sources:

```bash
python tools/download_dataset.py
```

**Features**:

- Downloads from URLs specified in `data/recitersAll.json`
- Supports multiple reciter sources
- Progress tracking with tqdm
- Automatic retry on failed downloads
- Respects server rate limits
- Downloads specific Surah ranges (1, 78-114 by default)

**Configuration Files**:

- `data/recitersAll.json`: Complete reciter database with download URLs
- `data/surahs.json`: Surah information with Ayah counts

#### Prepare Data Splits

Organizes downloaded data into training and testing splits:

```bash
python tools/prepare_data.py
```

**Process Flow**:

1. **Load Configuration**: Read split configuration from `data/dataset_splits.json`
2. **Validate Reciters**: Check availability against `data/recitersAll.json`
3. **Clean Directories**: Remove existing train/test directories for fresh start
4. **Copy Training Data**: Organize training reciters by configured Surah ranges
5. **Copy Testing Data**: Organize testing reciters by configured Surah ranges
6. **Update Metadata**: Generate `data/reciters.json` for training pipeline

**Configuration Example** (`data/dataset_splits.json`):

```json
{
  "training": [
    "Mishary Alafasy",
    "Abdul Rahman Al-Sudais",
    "Maher Al Mueaqly"
  ],
  "testing": [
    "Unknown Reciter 1",
    "Test Reciter 2"
  ],
  "train_data_range": "1-1",      # Surah 1 (Al-Fatihah)
  "test_data_range": "78-114",    # Short Surahs for testing
  "n_training_reciters": 3,
  "n_testing_reciters": 2
}
```

## Data Management

### File Naming Conventions

#### Ayah Audio Files

- **Format**: `SSSAAA.mp3`
- **SSS**: 3-digit Surah number (001-114)
- **AAA**: 3-digit Ayah number (001-286)
- **Examples**:
  - `001001.mp3` = Surah 1, Ayah 1 (Bismillah)
  - `002255.mp3` = Surah 2, Ayah 255 (Ayat al-Kursi)
  - `112001.mp3` = Surah 112, Ayah 1 (Qul Huwa Allah)

#### Timestamped Operations

- **Format**: `YYYYMMDD_HHMMSS_operation`
- **Examples**:
  - `20240306_143208_preprocess`
  - `20240306_152417_train`
  - `20240306_163045_test`

#### Output Organization

Each operation creates comprehensive structured outputs:

```
operation_YYYYMMDD_HHMMSS/
├── primary_data/          # Main results (features, models, predictions)
├── metadata/              # Configuration and run information
├── summaries/             # Human-readable reports
├── visualizations/        # Charts and analysis plots
└── debug/                 # Detailed processing information (debug mode)
```

### Configuration Files Management

#### Quran Text Data (`data/quran.json`)

Complete Quran text structured for Ayah identification:

```json
[
  {
    "id": 1,
    "name": "الفاتحة",
    "transliteration": "Al-Fatihah",
    "translation": "The Opening",
    "unicode": "🕌",
    "verses": [
      {
        "id": 1,
        "text": "بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ"
      },
      {
        "id": 2,
        "text": "الْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ"
      }
    ]
  }
]
```

#### Reciter Metadata

**`data/recitersAll.json`** - Master reciter database:

```json
{
  "Mishary Alafasy": {
    "nationality": "Kuwait",
    "flagUrl": "https://flags.example.com/kw.png",
    "imageUrl": "https://images.example.com/mishary.jpg",
    "servers": [
      "https://server1.example.com/mishary/",
      "https://server2.example.com/mishary/"
    ]
  }
}
```

**`data/reciters.json`** - Training-specific metadata (auto-generated):

```json
{
  "Mishary Alafasy": {
    "nationality": "Kuwait",
    "flagUrl": "https://flags.example.com/kw.png",
    "imageUrl": "https://images.example.com/mishary.jpg",
    "servers": ["https://server1.example.com/mishary/"]
  }
}
```

### Symlink Management

The system maintains convenience symlinks for easy access:

- **`processed/train/latest`** → most recent preprocessing run
- **`processed/test/latest`** → most recent test preprocessing
- **`models/latest`** → most recent training run

**Benefits**:

- Scripts can reference "latest" without knowing specific timestamps
- Backward compatibility when upgrading models
- Easy rollback to previous versions

### GPU Support and Optimization

#### Automatic GPU Detection

```python
# System automatically detects and utilizes available GPU
device = torch.device("cuda" if torch.cuda.is_available() and USE_GPU else "cpu")
```

**GPU Benefits by Component**:

- **Audio Preprocessing**: 2-3x speedup with PyTorch tensor operations
- **BLSTM Training**: 5-10x speedup for neural network operations
- **Whisper ASR**: 3-5x speedup for speech recognition
- **Batch Processing**: Significant improvement for large datasets

#### Memory Management

```python
# Configuration for memory optimization
BATCH_SIZE = 32          # Reduce if GPU memory limited
BLSTM_SEQUENCE_LENGTH = 16  # Adjust based on available memory
```

**Memory Requirements**:

- **Training**: 4-8 GB GPU memory for BLSTM with default settings
- **Inference**: 2-4 GB GPU memory for real-time processing
- **Whisper**: 1-2 GB GPU memory for ASR processing

## Troubleshooting

### Common Issues and Solutions

#### 1. FFmpeg Installation Problems

**Issue**: `FileNotFoundError: 'ffmpeg' executable not found`

**Solutions**:

```bash
# Ubuntu/Debian
sudo apt update && sudo apt install ffmpeg

# macOS with Homebrew
brew install ffmpeg

# Windows - Add to PATH after downloading from https://ffmpeg.org/
# Verify installation:
ffmpeg -version
```

**Test FFmpeg Integration**:

```python
python -c "import subprocess; subprocess.run(['ffmpeg', '-version'])"
```

#### 2. CUDA/GPU Detection Issues

**Issue**: GPU not detected or CUDA errors

**Diagnostic Commands**:

```bash
# Check CUDA availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# Check GPU details
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"

# Check CUDA version
nvcc --version
```

**Solutions**:

```bash
# Install/reinstall PyTorch with CUDA
pip uninstall torch torchaudio
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# Disable GPU if problematic
# In config/config.py: USE_GPU = False
```

#### 3. Model Loading Failures

**Issue**: Model files corrupted or incompatible

**Diagnostic Steps**:

```bash
# Check model directory structure
ls -la models/latest/
ls -la models/*/

# Verify model file integrity
python -c "import joblib; print(joblib.load('models/latest/model_random_forest.joblib'))"
```

**Solutions**:

```bash
# Retrain if model corrupted
python scripts/train.py --model-type random_forest

# Use specific model version
python scripts/predict.py --model-file-id 20240306_152417_train --audio test.mp3

# Clear corrupted models
rm -rf models/CORRUPTED_TIMESTAMP_train/
```

#### 4. Memory Issues

**Issue**: Out of memory during processing

**Solutions**:

```python
# Reduce batch sizes in config/config.py
BATCH_SIZE = 16          # Reduce from 32
BLSTM_SEQUENCE_LENGTH = 8  # Reduce from 16

# Disable augmentation to reduce memory usage
python scripts/preprocess.py --no-augment

# Process smaller chunks
# Split large datasets into smaller batches
```

#### 5. Audio Processing Errors

**Issue**: Audio files not loading or processing failures

**Diagnostic Commands**:

```python
# Test audio loading
python -c "import librosa; data, sr = librosa.load('test.mp3'); print(f'Loaded: {len(data)} samples at {sr} Hz')"

# Check audio file properties
python -c "import soundfile as sf; info = sf.info('test.mp3'); print(info)"
```

**Solutions**:

- **Convert audio format**: `ffmpeg -i input.m4a -ar 22050 -ac 1 output.wav`
- **Check file permissions**: `chmod 644 audio_files/*.mp3`
- **Verify file integrity**: Re-download or re-encode corrupted files

#### 6. Server Startup Issues

**Issue**: Server fails to start or models don't load

**Diagnostic Steps**:

```bash
# Test server components individually
python -c "from server.utils.model_loader import initialize_models; print(initialize_models())"
python -c "from server.utils.quran_data import get_raw_quran_data; print(len(get_raw_quran_data()))"

# Check server logs
python -m server.app --debug  # Enable detailed logging

# Test health endpoint
curl http://localhost:5000/health
```

**Solutions**:

- **Missing dependencies**: `pip install -r server/requirements.txt`
- **Missing data files**: Ensure `data/quran.json` and `data/reciters.json` exist
- **Port conflicts**: Use different port `python -m server.app --port 8080`

### Debug Mode Usage

Enable comprehensive debugging for issue diagnosis:

```bash
# Server debug mode - saves all requests/responses
python -m server.app --debug

# Check debug data
ls -la api-responses/getReciter/
ls -la api-responses/getAyah/

# Script debugging with verbose logging
export PYTHONPATH=.
python scripts/train.py  # Automatically includes debug logging
```

**Debug Features**:

- **Request/Response Logging**: All API calls saved to `api-responses/`
- **Detailed Error Traces**: Full stack traces with local variables
- **Processing Timing**: Step-by-step timing information
- **Model State Inspection**: Internal model state and parameters

### Performance Optimization

#### System Requirements

**Minimum Requirements**:

- **CPU**: 4 cores, 2.5 GHz
- **RAM**: 8 GB
- **Storage**: 50 GB free space
- **GPU**: Optional, GTX 1060 or better

**Recommended Requirements**:

- **CPU**: 8+ cores, 3.0+ GHz
- **RAM**: 16+ GB
- **Storage**: 100+ GB SSD
- **GPU**: RTX 3060 or better with 8+ GB VRAM

#### Performance Tuning

```python
# config/config.py optimizations
USE_GPU = True               # Enable GPU acceleration
BATCH_SIZE = 64             # Increase for better GPU utilization
N_ESTIMATORS = 50           # Reduce for faster Random Forest training
EPOCHS = 25                 # Reduce for faster BLSTM training

# Disable augmentation for faster preprocessing
python scripts/preprocess.py --no-augment

# Use smaller feature sets for faster processing
N_MFCC = 13                 # Reduce from 32
N_MEL_BANDS = 32            # Reduce from 64
```

## Acknowledgments

This project builds upon excellent open-source libraries and tools:

### Core Dependencies

- **Audio Processing**: [librosa](https://librosa.org/), [soundfile](https://python-soundfile.readthedocs.io/), [pydub](https://pydub.com/)
- **Machine Learning**: [scikit-learn](https://scikit-learn.org/), [PyTorch](https://pytorch.org/), [TensorFlow](https://tensorflow.org/)
- **Speech Recognition**: [OpenAI Whisper](https://openai.com/research/whisper), [Tarteel Whisper Models](https://huggingface.co/tarteel-ai)
- **Text Processing**: [rapidfuzz](https://rapidfuzz.github.io/RapidFuzz/) for efficient fuzzy string matching
- **Web Framework**: [Flask](https://flask.palletsprojects.com/) for API development
- **CLI and Progress**: [Rich](https://rich.readthedocs.io/) for enhanced terminal output and progress bars

### Special Recognition

- **Tarteel.ai**: For providing specialized Arabic Whisper models optimized for Quranic text
- **Librosa Team**: For comprehensive audio analysis tools that form the backbone of our feature extraction
- **scikit-learn Contributors**: For robust, well-tested machine learning algorithms
- **PyTorch Team**: For the deep learning framework enabling our BLSTM implementation

---

This comprehensive system provides a complete solution for Quran reciter and verse identification with robust machine learning pipelines, production-ready API endpoints, and extensive tooling for development and maintenance. The modular architecture allows for easy extension and customization while maintaining high performance and reliability standards.
